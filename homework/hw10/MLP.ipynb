{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-19T11:30:36.673334Z",
     "iopub.status.busy": "2021-05-19T11:30:36.672690Z",
     "iopub.status.idle": "2021-05-19T11:30:43.495621Z",
     "shell.execute_reply": "2021-05-19T11:30:43.494984Z"
    },
    "id": "0trJmd6DjqBZ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7NAbSZiaoJ4z"
   },
   "source": [
    "Download dữ liệu chữ số viết tay MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-19T11:30:43.501155Z",
     "iopub.status.busy": "2021-05-19T11:30:43.500509Z",
     "iopub.status.idle": "2021-05-19T11:30:44.026366Z",
     "shell.execute_reply": "2021-05-19T11:30:44.026834Z"
    },
    "id": "JqFRS6K07jJs"
   },
   "outputs": [],
   "source": [
    "# Chuẩn bị dữ liệu\n",
    "from tensorflow.keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# Chuyển đổi sang định dạng float32.\n",
    "x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)\n",
    "# Chuẩn hóa ảnh từ from [0, 255] to [0, 1].\n",
    "x_train, x_test = x_train / 255., x_test / 255.\n",
    "\n",
    "x_train = x_train.reshape(-1, 28*28)\n",
    "x_test = x_test.reshape(-1, 28*28)\n",
    "\n",
    "x_train, x_test, y_train, y_test = torch.from_numpy(x_train), torch.from_numpy(x_test), torch.from_numpy(y_train).type(torch.LongTensor), torch.from_numpy(y_test).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "jS1f1TsVxoj6"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "batch_size = 16\n",
    "\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "trainloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "test_dataset = TensorDataset(x_test, y_test)\n",
    "testloader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Ps36LD2btm0W"
   },
   "outputs": [],
   "source": [
    "num_features = 784\n",
    "n_hidden_1 = 512\n",
    "n_hidden_2 = 128\n",
    "n_hidden_3 = 32\n",
    "num_classes = 10\n",
    "\n",
    "epoches = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ar0KFOo-mjsn"
   },
   "source": [
    "Sử dụng các tham số ở trên để xây dựng mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-19T11:30:44.902515Z",
     "iopub.status.busy": "2021-05-19T11:30:44.901796Z",
     "iopub.status.idle": "2021-05-19T11:30:45.195908Z",
     "shell.execute_reply": "2021-05-19T11:30:45.195388Z"
    },
    "id": "h3IKyzTCDNGo"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, layer_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(layer_size[0], layer_size[1])\n",
    "        self.fc2 = nn.Linear(layer_size[1], layer_size[2])\n",
    "        self.fc3 = nn.Linear(layer_size[2], layer_size[3])\n",
    "        self.output = nn.Linear(layer_size[3], layer_size[4])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "net = Net([784, 512, 128, 32, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "t9ns5fidtz-t"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-05-19T11:30:45.235727Z",
     "iopub.status.busy": "2021-05-19T11:30:45.235098Z",
     "iopub.status.idle": "2021-05-19T11:31:05.875247Z",
     "shell.execute_reply": "2021-05-19T11:31:05.875663Z"
    },
    "executionInfo": {
     "elapsed": 12045,
     "status": "ok",
     "timestamp": 1630204831765,
     "user": {
      "displayName": "Nam Cao Hải",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCmrl3X4wfKjF82UzLyBPTNB6px2ty7jZ8llyL=s64",
      "userId": "02198006735637931468"
     },
     "user_tz": -420
    },
    "id": "i-2pkctU_Ci7",
    "outputId": "ad91b93e-0f6d-492f-f9de-568d383321cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 1.469\n",
      "[2,  2000] loss: 0.318\n",
      "[3,  2000] loss: 0.210\n",
      "[4,  2000] loss: 0.152\n",
      "[5,  2000] loss: 0.114\n",
      "[6,  2000] loss: 0.092\n",
      "[7,  2000] loss: 0.073\n",
      "[8,  2000] loss: 0.064\n",
      "[9,  2000] loss: 0.053\n",
      "[10,  2000] loss: 0.046\n",
      "[11,  2000] loss: 0.040\n",
      "[12,  2000] loss: 0.032\n",
      "[13,  2000] loss: 0.027\n",
      "[14,  2000] loss: 0.023\n",
      "[15,  2000] loss: 0.019\n",
      "[16,  2000] loss: 0.015\n",
      "[17,  2000] loss: 0.013\n",
      "[18,  2000] loss: 0.011\n",
      "[19,  2000] loss: 0.008\n",
      "[20,  2000] loss: 0.006\n",
      "[21,  2000] loss: 0.005\n",
      "[22,  2000] loss: 0.005\n",
      "[23,  2000] loss: 0.003\n",
      "[24,  2000] loss: 0.003\n",
      "[25,  2000] loss: 0.003\n",
      "[26,  2000] loss: 0.002\n",
      "[27,  2000] loss: 0.002\n",
      "[28,  2000] loss: 0.001\n",
      "[29,  2000] loss: 0.001\n",
      "[30,  2000] loss: 0.001\n",
      "[31,  2000] loss: 0.001\n",
      "[32,  2000] loss: 0.001\n",
      "[33,  2000] loss: 0.001\n",
      "[34,  2000] loss: 0.001\n",
      "[35,  2000] loss: 0.001\n",
      "[36,  2000] loss: 0.001\n",
      "[37,  2000] loss: 0.001\n",
      "[38,  2000] loss: 0.001\n",
      "[39,  2000] loss: 0.001\n",
      "[40,  2000] loss: 0.001\n",
      "[41,  2000] loss: 0.001\n",
      "[42,  2000] loss: 0.001\n",
      "[43,  2000] loss: 0.001\n",
      "[44,  2000] loss: 0.001\n",
      "[45,  2000] loss: 0.000\n",
      "[46,  2000] loss: 0.000\n",
      "[47,  2000] loss: 0.000\n",
      "[48,  2000] loss: 0.000\n",
      "[49,  2000] loss: 0.000\n",
      "[50,  2000] loss: 0.000\n",
      "[51,  2000] loss: 0.000\n",
      "[52,  2000] loss: 0.000\n",
      "[53,  2000] loss: 0.000\n",
      "[54,  2000] loss: 0.000\n",
      "[55,  2000] loss: 0.000\n",
      "[56,  2000] loss: 0.000\n",
      "[57,  2000] loss: 0.000\n",
      "[58,  2000] loss: 0.000\n",
      "[59,  2000] loss: 0.000\n",
      "[60,  2000] loss: 0.000\n",
      "[61,  2000] loss: 0.000\n",
      "[62,  2000] loss: 0.000\n",
      "[63,  2000] loss: 0.000\n",
      "[64,  2000] loss: 0.000\n",
      "[65,  2000] loss: 0.000\n",
      "[66,  2000] loss: 0.000\n",
      "[67,  2000] loss: 0.000\n",
      "[68,  2000] loss: 0.000\n",
      "[69,  2000] loss: 0.000\n",
      "[70,  2000] loss: 0.000\n",
      "[71,  2000] loss: 0.000\n",
      "[72,  2000] loss: 0.000\n",
      "[73,  2000] loss: 0.000\n",
      "[74,  2000] loss: 0.000\n",
      "[75,  2000] loss: 0.000\n",
      "[76,  2000] loss: 0.000\n",
      "[77,  2000] loss: 0.000\n",
      "[78,  2000] loss: 0.000\n",
      "[79,  2000] loss: 0.000\n",
      "[80,  2000] loss: 0.000\n",
      "[81,  2000] loss: 0.000\n",
      "[82,  2000] loss: 0.000\n",
      "[83,  2000] loss: 0.000\n",
      "[84,  2000] loss: 0.000\n",
      "[85,  2000] loss: 0.000\n",
      "[86,  2000] loss: 0.000\n",
      "[87,  2000] loss: 0.000\n",
      "[88,  2000] loss: 0.000\n",
      "[89,  2000] loss: 0.000\n",
      "[90,  2000] loss: 0.000\n",
      "[91,  2000] loss: 0.000\n",
      "[92,  2000] loss: 0.000\n",
      "[93,  2000] loss: 0.000\n",
      "[94,  2000] loss: 0.000\n",
      "[95,  2000] loss: 0.000\n",
      "[96,  2000] loss: 0.000\n",
      "[97,  2000] loss: 0.000\n",
      "[98,  2000] loss: 0.000\n",
      "[99,  2000] loss: 0.000\n",
      "[100,  2000] loss: 0.000\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "rloss = 0\n",
    "for epoch in range(epoches):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # load input và labels\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        # print(labels.shape)\n",
    "        # print(outputs.shape)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "        rloss = running_loss\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "qTG2aSROu4kK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 98 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# do đang thực hiện việc dự đoán nên ko cần tính đạo hàm\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        inputs, labels = data\n",
    "        # chạy hàm dự đoán\n",
    "        outputs = net(inputs)\n",
    "        # the class với giá trị xác suất cao nhất là đâu ra dự đoán\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHXvZHDXOhMW"
   },
   "source": [
    "## Save and load model<br>\n",
    "Trình bày 1 trong các các lưu model và load model trong PyTorch\n",
    "\n",
    "1.   Lưu model<br>\n",
    "```\n",
    "torch.save(model.state_dict(), PATH)\n",
    "```\n",
    "trong đó PATH là đường dẫn tự định nghĩa\n",
    "2.   Load model <br>\n",
    "\n",
    "*   Trước tiên phải định nghĩa model trước. Model được định nghĩa phải giống hệt với model đã được lưu lại. Như ví dụ trong bài này, thì sẽ thực hiện như sau: \n",
    "```\n",
    "model = Net()\n",
    "```\n",
    "*   Load trọng số đã được học vào mô hình<br>\n",
    "```\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "# vô hiệu hóa các layer như Dropout hay BatchNorm \n",
    "model.eval()\n",
    "```\n",
    "3. Có thể tham khảo thêm các phương pháp lưu và load model tại: https://pytorch.org/tutorials/beginner/basics/saveloadrun_tutorial.html\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "lhSsevChOxXE"
   },
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': net.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': rloss / 2000, \n",
    "}, 'checkpoint.pth')\n",
    "\n",
    "torch.save(net, 'net.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net([784, 512, 128, 32, 10])\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) \n",
    "\n",
    "checkpoint = torch.load('checkpoint.pth')\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch_da_luu = checkpoint['epoch']\n",
    "loss_da_luu = checkpoint['loss']"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MLP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv (3.9.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
